# scraper/main_scraper.py

import requests
import random
from pathlib import Path
from bs4 import BeautifulSoup # We'll need this for parsing later

# --- File Definitions ---
# This file is generated by proxy_tester.py
WORKING_PROXIES_LIST_FILE = Path(__file__).parent / "working_proxies_list.txt"

def load_working_proxies(filepath: Path) -> list:
    """Loads the list of working proxies from the file."""
    if not filepath.exists():
        print(f"❌ Proxy list file not found at: {filepath}")
        print("Please run 'proxy_tester.py' first to generate the list.")
        return []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        proxies = [line.strip() for line in f if line.strip()]
    
    print(f"✅ Loaded {len(proxies)} working proxies from the list.")
    return proxies

def make_request_with_proxy(url: str, proxies_list: list, max_retries: int = 5):
    """
    Makes a request to a URL using a random proxy from the list.
    If a proxy fails, it's removed from the list and another is tried, up to max_retries.
    """
    if not proxies_list:
        print("Proxy list is empty. Cannot make a request.")
        return None

    retries = 0
    while retries < max_retries and proxies_list:
        try:
            # 1. Randomly choose a proxy from our working list
            proxy = random.choice(proxies_list)
            print(f"[*] Attempting request with proxy: {proxy} ({retries + 1}/{max_retries})")

            # 2. Set up the proxies dictionary for the request
            proxies_dict = {"http": proxy, "https": proxy}
            
            # 3. Make the request
            response = requests.get(url, proxies=proxies_dict, timeout=15)
            
            # 4. If the request was not successful, raise an exception to trigger the retry logic
            response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)
            
            print(f"✅ Successfully fetched URL: {url}")
            return response # Success! Return the response object.

        except requests.exceptions.RequestException as e:
            print(f"❌ Proxy {proxy} failed. Error: {e.__class__.__name__}. Removing from list.")
            # Remove the failing proxy so we don't use it again in this run
            proxies_list.remove(proxy)
            retries += 1
            if not proxies_list:
                print("All proxies have failed.")
                break
    
    print(f"Failed to fetch URL after {max_retries} retries.")
    return None

if __name__ == "__main__":
    # --- The Main Scraping Logic ---
    
    # 1. Load our pre-tested proxies
    working_proxies = load_working_proxies(WORKING_PROXIES_LIST_FILE)

    if not working_proxies:
        exit() # Exit if we have no proxies to work with

    # 2. Define the target website to scrape
    # We use quotes.toscrape.com, a website designed for practicing scraping.
    TARGET_URL = "http://quotes.toscrape.com/"

    # 3. Make the request using our robust proxy-rotating function
    page_response = make_request_with_proxy(TARGET_URL, working_proxies)

    # 4. Process the response if it was successful
    if page_response:
        print("\n--- Scraping Result ---")
        print(f"Status Code: {page_response.status_code}")
        
        # Here's where you would parse the content.
        # For now, let's just show the page title to prove it worked.
        soup = BeautifulSoup(page_response.text, 'html.parser')
        title = soup.find('title')
        if title:
            print(f"Page Title: {title.text}")
        else:
            print("Could not find page title.")
        print("="*25)